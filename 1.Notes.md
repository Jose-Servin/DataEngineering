# Course: Understanding Data Engineering

## The Data Workflow

1. Data collection and storage (Data Engineer)
2. Data Preparation (Data Scientist)
3. Exploration and Visualization (Data Analyst)
4. Experimentation and Prediction (Data Scientist)

## Data Engineer Responsibilities

* Ingest data from various sources. (Build and maintain Data Pipelines)
* Optimize database for analysis.
* Remove corrupted data.
* Develop, construct, test and maintain data architecture.

## The Data Pipeline

A data pipeline moves "raw data"into various staging locations in order for ETL process framework to take place. ETL is a data pipeline framework that stands for Extract, Transform and Load.

## Data Structures

Data Structures represent both excel, csv style formats as well as the relational database which can be queried using SQL. Data structures can also have relationships which in turn create links between data sources.

Semi-structured data is relatively easy to search and organize but work is needed to extract value and create relationships. Examples include JSON, XML and YAML (NoSQL).

Unstructured data is data that cannot be contained in rows x columns. Usually this can be text, sound, videos, images etc.

Note that about 20% of data is structured.

## Data Warehouses and Data Lakes

Data Lake:

* Where all raw data, from various sources, is stored.
* Unprocessed and messy.
* Does not enforce a certain data structure; stores all data structures.
* Can be cost effective
* Difficult to analyze.
* Requires an up-to-date catalog.

Data Lakes implement the use of "catalogs" which hold information about the source of the data, where the data is being used, who owns the data and how often the data is updated. Having a catalog is considered best-practice since it introduces a process to Data Lakes.

Data Warehouse (A type of Database)

* Stored specific data for a specific use.
* Stored mainly structured data.
* More costly to update.
* Optimized for analytics.

## Processing Data

Data Processing: converting raw data into meaningful information.

Data Engineers process data by:

* Cleaning data
* storing data in the optimal structure.
* create views on top of database tables.
* Optimize the performance of the database (one option is using indexes)
* Rejecting "bad" data and handling metadata.

## Scheduling Data

In the context of Data Engineering, scheduling refers to small tasks running in a specific order in order to achieve usable data.

Batches refers to grouped data being updated at set intervals. This is often the cheaper option.
Streamed Data refers to individual record updates being sent as soon as any change is made.

## Parallel Computing

The foundation of data processing tools.

Parallel computing is necessary because of memory power and processing power constraints.

How it works:

1. tasks are split into smaller tasks.

Parallel computing pros vs cons

* We get extra processing power and a smaller memory footprint per computer.
* More cost added on since we are now using more resources.
* communication time is required to aggregate smaller tasks results back into main task objective.

## Cloud Computing

Main advantage we see with moving to the cloud is service coverage, db reliability and security.

AWS Specifics:

* File storage: AWS S3
* Computation/Processing: AWS EC2
* Database: AWS RDS

We can also use the multi-cloud model in order to mitigate risk however there can be incompatibility issues and governance risks.

# Course: Introduction to Data Engineering

In it's simplest form, a data pipeline can be thought of as:

<img src="images/simple_data_pipeline.png" alt="a visual aid for a data pipeline">

It is the role of the Data Engineer to join, clean and organize data before loading it into a destination analytics database.

For this example, Apache Spark is the HUB for all "dirty" data.

All of the commands needed to run are held together using Apache Airflow. Which is the scheduler in this process. Essentially,Airflow makes sure a job runs in a specific order and resolves all dependencies.

## Cloud Providers at a glance

The top 3 services Cloud Providers provide are:

1. Storage services allow us to upload files to the cloud. -  AWS S3
2. Computation - hosting a web server or VM - AWS EC2
3. Databases - hold structured information - AWS RDS

## Databases

One SQL Database architecture that is common to see is the Star Schema. Which by definition is "one or more fact tables referencing any number of dimension tables."

 <img src="images/star_schema.png" alt="simple star schema visual">

## What is Parallel Computing?

The why? Parallel computing at it's simplest core, is the management of memory and processing power.

By "memory" we also mean the partitioning of data into smaller subsets. Therefore, the memory footprint per computer is fairly small.

In other words, we can think of parallel computing as "splitting a BIG task into smaller tasks distributed over several computers."

What are the Risks?

Because we are splitting up a task, there needs to be constant and successfully communication between processes.

## Parallel computing frameworks

Apache Hadoop

* HDFS - a file system that is distributed amongst different computers.
* MapReduce - a big data processing paradigm. Functionality can be easily understood by the "split one BIG task into smaller task" example. The data and workload is distributed between several processing units.

    One flaw of MapReduce is it's challenging to write MapReduce jobs.
* Hive -  a data warehouse software project that provides data querying and analysis.
* Apache Spark - distributes data processing tasks between clusters of computers.
    A main difference between Spark and MapReduce is that Spark will try to maintain as much processing as possible in memory.

## Examining Spark

Spark's framework relies on "Resilient Distributed Datasets (RDD)."

In it's most basic definition form, this just means each dataset is divided into logical partitions which can be then computed on different nodes.

To RDD's we can apply:

1. Transformations `.map()`, `.filter()`.
2. Actions `.count()`, `.first()`.

The programming interface that is uses most commonly with Spark is PySpark. PySpark is the python interface to Spark. PySpark syntax follows closely the "DataFrame abstraction" chaining.

## Workflow scheduling frameworks

* DAG: Directed Acyclic Graph

DAGS's are the core concept of Airflow. A DAG represents a collection of tasks, and dependencies, organized in a way that establishes a relationship when executed.

DAG's are not the definition of said tasks, instead they are only addressing the "how are these task related", "what order to run them", "how many times to retry etc."

What is Airflow? a work scheduling framework.

```mermaid
flowchart LR
   start_cluster --> ingest_customer_data & ingest_product_data--> enrich_customer_data
```

## Extract Transform and Load (ETL)

### Extracting Data

Data can come from various sources such as csv files, databases, API's. The purpose of extracting data is to bring it to a "ready for processing" stage.

A widely used file format is the JSON (Java-Script Object Notation) which holds similar structure to the python dictionary. The information in the JSON file is considered "semi-structured" since extraction uses a series of key-value pairs.

In python the `json` package is used to parse JSON data.

A simple JSON example:

```python
import requests

# Fetch the Hackernews post
resp = requests.get("https://hacker-news.firebaseio.com/v0/item/16222426.json")

# Print the response parsed as JSON
print(resp.json())

# Assign the score of the test to post_score
post_score = resp.json()["score"]
print(post_score)
```

A simple DB extraction:

```python
# Function to extract table to a pandas DataFrame
def extract_table_to_pandas(tablename, db_engine):
    query = "SELECT * FROM {}".format(tablename)
    return pd.read_sql(query, db_engine)

# Connect to the database using the connection URI
connection_uri = "postgresql://repl:password@localhost:5432/pagila"
db_engine = sqlalchemy.create_engine(connection_uri)

# Extract the film table into a pandas DataFrame
extract_table_to_pandas("film", db_engine)

# Extract the customer table into a pandas DataFrame
extract_table_to_pandas("customer", db_engine)
```

### Transform

There are many transformation we can apply to data. Some include:

* translation of code values ex. New York transforms to NY
* parsing columns to extract chunks. ex. splitting username and domain from an email.

Example: splitting the email column using Pandas

```python
# a pandas DF
my_data 

# a new DF with two columns, one with left side split and one with the right side split
split_email = my_data.email.str.split("@", expand=True)

my_data = my_data.assign(
    username = split_email[0],
    domain = split_email[1]
)
```

Example: transforming data in PySpark

```python
import pyspark.sql
spark = pyspark.sql.SparkSession.builder.getOrCreate()

spark.read.jdbc(
    "jdbc:postgresql://localhost:5432/name",
    "my_table_name",
    properties = {
        "user":"repl",
        "password":"mypass"
    }
)

```

Example: joining in PySpark

```python
customer_df
ratings_df

# Group by ratings
ratings_per_customer = ratings_df.groupBy("customer_id").mean("ratings")

# join aggregated table to customer_df

customer_df.join(
    ratings_per_customer,
    customer_df.customer_id == ratings_per_customer.customer_id
)

```

### Loading

Two distinct database types:

1. OLAP - best for aggregate queries
2. OLTP - best for applications

In the ETL pipeline, we often see, Massively Parallel Processing Databases (MPP Databases). This is a column-oriented DB that is optimized for analytics and runs on a distributed node fashion.

Example loading into RedShift:

Note, for this type of loading proceedure, csv files are not recommended and will go against best practices. Instead, we need to use `parquet` files.

Step 1: load from file to columnar storage format.

```python

# using pandas
df.to_parquet("./s3://path/to/bucket/customer.parquet")

# using PySpark
df.write.parquet("./s3://path/to/bucket/customer.parquet")
```

Step 2: connect to RedShift using PostgreSQL URI

```postgresql
COPY custmer
FROM "s3::/path/to/bucket/customer.parquet"
FORMAT as parquet
```

Example: Load the data into PostgreSQL DB

```python
customer_data

# Load
transform_df.to_sql(
    "customer_data",
    db_engine,
    schema = "store",
    if_exists = "replace"
)
```

## The ETL Pipeline Full Picture

First, we must define out ETL functions. This includes various extracting, cleaning and one final etl function.

```python
def extract_table_from_db():
    return pd.read_sql

def split_columns_transform(df):
    pass
    # performs action on DF
    # we will need to re-assign df variable

def load_into_dwh(df):
    return pd.to_sql
    # this is an "action" on how to load data into PostgreSQL DB

db_configurations = {}
def etl():

    # Extract
    my_df = extract_table_from_db()

    # Transform
    my_df = split_columns_transform(my_df)
    
    # Load
    load_into_dwh(my_df)

```

To ensure this ETL and other processes run at a set schedule and with instructions on what to do if an error occurs, we will be using AirFLow.

Example: building a DAG

```python
from airflow.models import DAG
from airflow.operators.python_operator import PythonOperator

# first we define out DAG process
dag = DAG(
    dag_id = "sample",
    ...,
    schedule_interval = "0 0 * * *"
)

# next we define out tasks to perform

etl_task = PythonOperator(
    task_id = "etl_task",
    # this is the function to call
    python_callable = etl,
    dag= dag
)

# Next we build our streams: set_upstream or set_downstream

elt_task.set_upstream(wait_for_this_task)

# set_upstream means left process will wait for argument process to complete.

```

After out DAG is completed, we save it as `etl_dag.py` file and into the AirFlow DAG folder `~/airflow/dags`. The AirFlow UI will detect this new DAG and show it to us on the webpage.
